# Assessment System Upgrade Plan

## Current System Overview

The current assessment system:
- Uses OpenAI's GPT-4 through the Node API to generate free-form questions
- Evaluates text responses via another OpenAI prompt through the Node API
- Stores minimal data about assessments in the MongoDB backend
- Uses a 50% passing threshold
- Questions are a mix of multiple choice, checkbox, and textarea inputs
- Questions are generated on-demand but not systematically stored
- Architecture separates concerns: Node API handles AI interactions, .NET backend with MongoDB handles data persistence

## New Requirements

Based on the `Assesment_prompt.md` file:
1. Generate a bank of 200 multiple-choice questions 
2. Two user types with different assessments:
   - Cleaners/Home Managers: 10 random questions (from 50 questions in 4 categories)
   - Caregivers: 30 random questions (from 150 questions in 8 categories)
3. Store questions, options, and answers in the database
4. Each question needs a unique ID
5. Increase passing threshold to 70%
6. Record user responses for data quality and audit purposes

## Planned System Changes

### 1. Database Schema Updates

**New Tables/Collections:**

#### `QuestionBank` Collection
- `id`: Unique identifier for the question
- `category`: Category of the question (e.g., "Respecting Client Privacy", "Basic Caregiving Skills")
- `userType`: Target user type ("Cleaner", "Caregiver", or "Both")
- `question`: The question text
- `options`: Array of 4 options (A, B, C, D)
- `correctAnswer`: The correct answer (A, B, C, D)
- `explanation`: Explanation of the correct answer
- `createdAt`: Timestamp
- `updatedAt`: Timestamp
- `active`: Boolean flag to enable/disable questions

#### `UserAssessment` Collection (Expanded)
- `id`: Unique identifier for the assessment attempt
- `userId`: User who took the assessment
- `userType`: "Cleaner" or "Caregiver"
- `startTimestamp`: When the assessment began
- `endTimestamp`: When the assessment was submitted
- `score`: Percentage score (0-100)
- `passed`: Boolean indicator (score >= 70%)
- `questions`: Array of:
  - `questionId`: Reference to QuestionBank
  - `question`: Question text (snapshot for audit)
  - `options`: Array of options (snapshot for audit)
  - `correctAnswer`: The correct answer (snapshot for audit)
  - `userAnswer`: User's selected answer
  - `isCorrect`: Boolean indicator
- `createdAt`: Timestamp
- `updatedAt`: Timestamp

### 2. System Component Modifications

#### a. Question Generation System (`openAIService.js` in Node API)

Current workflow:
1. Request questions based on provider type via Node API
2. OpenAI generates questions in free-form text
3. Parse into separate questions
4. Return to frontend or save minimal data to MongoDB via .NET backend

New workflow:
1. Create a specialized question generation script in the Node API that will:
   - Use the new prompt from `Assesment_prompt.md`
   - Generate all 200 questions in batches (to avoid token limits)
   - Parse responses into structured question objects with options and answers
   - Send the structured question data to the .NET backend API
   - Backend API stores questions in the MongoDB QuestionBank collection
2. Add admin functionality in the backend to review, edit, and approve questions
3. Implement a system to flag questions that need review based on user performance
4. Maintain the current architecture where Node API handles AI interactions and .NET backend handles data persistence

#### b. Assessment APIs

**Node API (AI Interaction Layer):**
- `POST /api/kyc/generate-question-bank` - Generate a batch of questions using OpenAI
- `POST /api/kyc/evaluate` - Evaluate results (temporarily until we move to multiple choice)
- `GET /api/kyc/question-stats` - Get statistics about question performance

**Backend .NET API (Data Layer):**
- `POST /api/QuestionBank` - Store questions generated by Node API
- `GET /api/assessment/questions/:userType` - Get a randomized test for the user type
- `POST /api/assessment/submit` - Submit assessment answers and calculate score
- `GET /api/assessment/results/:assessmentId` - Get detailed assessment results
- `GET /api/admin/questions` - Admin endpoint to manage question bank
- `POST /api/admin/questions` - Admin endpoint to add questions
- `PUT /api/admin/questions/:id` - Admin endpoint to edit questions
- `DELETE /api/admin/questions/:id` - Admin endpoint to delete questions

#### c. Frontend Assessment UI

UI changes:
- Update to support multiple-choice only format
- Add proper randomization of questions based on user type
- Update passing threshold messaging to 70%
- Add results view showing correct/incorrect answers with explanations
- Add assessment history view for users
- Add admin interface for question bank management

### 3. Data Migration and Initial Setup

1. Create a script in the Node API to generate the initial question bank:
   - Use the OpenAI prompt from `Assesment_prompt.md`
   - Generate questions in batches (50 at a time)
   - Store in temporary JSON files for review
   - Build an admin interface to review and edit questions
   - Send approved questions to the .NET backend API for storage in MongoDB

2. Implement database migration in the .NET backend:
   - Create new schema/collections in MongoDB
   - Define models and DTOs for questions and assessment results
   - Add endpoints to receive and store question bank data
   - Update existing assessment records with placeholder data for new fields

### 4. Assessment Process Flow

#### For Cleaners/Home Managers:
1. User initiates assessment
2. System randomly selects 10 questions from cleaner-related categories
3. User completes multiple-choice assessment
4. System calculates score (70% threshold)
5. All question and answer data saved to database
6. Results displayed to user

#### For Caregivers:
1. User initiates assessment
2. System randomly selects 30 questions covering all 8 categories 
   - Proportionally representing each category
   - Including cleaner-related questions
3. User completes multiple-choice assessment
4. System calculates score (70% threshold)
5. All question and answer data saved to database
6. Results displayed to user

### 5. Auditing and Analytics

New features:
1. Admin dashboard showing:
   - Assessment completion rates
   - Average scores by question category
   - Pass/fail rates
   - Individual question performance metrics
2. Question analysis tools:
   - Identify questions that are too easy (>90% correct)
   - Identify questions that are too difficult (<30% correct)
   - Identify potentially ambiguous questions

### 6. Implementation Plan

#### Phase 1: Question Bank Generation (Node API)
1. Update OpenAI service in Node API to use new prompt format
2. Create question generation script for multiple choice questions
3. Generate initial question batches
4. Build a temporary admin review interface for question quality control
5. Develop API endpoints to transmit approved questions to the .NET backend

#### Phase 2: Database and Backend (.NET)
1. Update MongoDB schema in .NET backend
2. Create models and DTOs for question bank and assessment results
3. Implement endpoints to receive and store question bank data
4. Develop logic for randomized question selection based on user type
5. Update assessment scoring logic to use 70% threshold

#### Phase 3: Frontend
1. Update assessment UI for multiple choice only format
2. Implement user type-specific question selection
3. Update results display with new scoring system
4. Create admin question bank management interface

#### Phase 4: Integration and API Communication
1. Establish robust communication between Node API and .NET backend
2. Implement error handling and retry mechanisms
3. Create monitoring for API health and performance
4. Test cross-service data flow and integrity

#### Phase 5: Testing and Refinement
1. Test assessment flow end-to-end
2. Verify data storage and retrieval across systems
3. Conduct user acceptance testing
4. Refine question bank based on initial results
5. Stress test API communication under load

#### Phase 6: Analytics and Reporting
1. Implement assessment analytics
2. Create administrative reports
3. Set up periodic question bank review process
4. Monitor system performance and refine as needed

## Technology Considerations

### Architecture
- Maintain current separation of concerns:
  - Node API handles OpenAI interactions for question generation
  - .NET backend with MongoDB handles data persistence
  - Frontend handles UI rendering and user interactions

### Database 
- MongoDB is the current database system (based on AssessmentDTO.cs)
- Question bank will be a separate collection from user assessment results
- Consider indexing on category and userType for efficient question retrieval
- Ensure proper data synchronization between Node API and MongoDB backend

### API Integration
- Keep current OpenAI integration in the Node API for question bank generation
- Create proper data transfer between Node API and .NET backend
- Consider implementing scheduled generation of new questions to expand bank over time
- Add validation rules to ensure questions meet quality standards

### Frontend
- Update assessment components to handle multiple-choice only format
- Create new admin interfaces for question management
- Implement proper progress tracking and timing for assessments

## Potential Challenges

1. **Initial Question Quality**: Generated questions may need human review and editing
2. **Database Performance**: Random question selection needs to be efficient
3. **User Experience**: Ensuring clear instructions and feedback during the assessment
4. **Data Integrity**: Proper validation to prevent cheating or data corruption
5. **Compliance**: Ensuring proper data handling for audit purposes
6. **API Coordination**: Managing the communication between Node API and .NET backend
7. **Data Synchronization**: Ensuring question data is properly transferred and stored
8. **Error Handling**: Robust error handling between services to prevent data loss

## Conclusion

This upgrade will transition the assessment system from a dynamic, AI-evaluated format to a more structured multiple-choice approach with comprehensive data storage. The change will improve consistency, enable better analytics, and support proper auditing of the assessment process. The 70% passing threshold will ensure higher quality standards for both cleaners and caregivers.

We will maintain the current architecture where:
- The Node API handles AI interactions with OpenAI for question generation
- The .NET backend with MongoDB handles data persistence for the question bank and assessment results
- The frontend handles user interactions and displays

This separation of concerns allows us to leverage the strengths of each component:
- Node.js for efficient API interactions and processing
- .NET backend for robust data storage and business logic
- MongoDB for flexible document storage of questions and assessment data

The implementation should be phased, starting with the Node API for question generation, followed by MongoDB schema and backend API updates, and finally the frontend enhancements. Each phase should include thorough testing to ensure system integrity and data quality across all system components.
